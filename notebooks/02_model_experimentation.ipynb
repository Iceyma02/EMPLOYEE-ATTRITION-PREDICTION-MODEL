{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7e76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n",
      "âœ… MLflow version: 3.9.0\n"
     ]
    }
   ],
   "source": [
    "# 02_model_experimentation.ipynb - Cell 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "project_path = r\"C:\\Users\\Icey_m_a\\Documents\\Icey\\Icey\\School\\Python\\Employee Attrition Prediction Model\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "# Import preprocessing\n",
    "from src.data.preprocessing import load_raw_data, create_preprocessing_pipeline, prepare_data\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "print(\"âœ… Setup complete\")\n",
    "print(f\"âœ… MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237ad724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading data...\n",
      "âœ… Dataset loaded successfully!\n",
      "   Shape: (1470, 35)\n",
      "   Attrition distribution:\n",
      "Attrition\n",
      "No     0.838776\n",
      "Yes    0.161224\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ðŸ”§ Creating preprocessing pipeline...\n",
      "âœ… Preprocessing pipeline created\n",
      "   Numerical features: 23\n",
      "   Categorical features: 7\n",
      "   Columns to drop: 4\n",
      "\n",
      "ðŸ”„ Preparing train/test split...\n",
      "\n",
      "ðŸ”„ Preparing data (train=80%, test=20%)...\n",
      "   Features shape: (1470, 30)\n",
      "   Target distribution: {0: 1233, 1: 237}\n",
      "   Train set: 1176 samples\n",
      "   Test set: 294 samples\n",
      "   Applying preprocessing...\n",
      "âœ… Data preparation complete!\n",
      "   Processed train shape: (1176, 44)\n",
      "   Processed test shape: (294, 44)\n",
      "   Total features: 44\n",
      "\n",
      "âœ… Preprocessor saved to ../models/preprocessor.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Load and prepare data\n",
    "print(\"ðŸ“‚ Loading data...\")\n",
    "df = load_raw_data()\n",
    "\n",
    "print(\"\\nðŸ”§ Creating preprocessing pipeline...\")\n",
    "preprocessor, drop_cols, num_cols, cat_cols = create_preprocessing_pipeline()\n",
    "\n",
    "print(\"\\nðŸ”„ Preparing train/test split...\")\n",
    "X_train, X_test, y_train, y_test, feature_names, fitted_preprocessor = prepare_data(\n",
    "    df, preprocessor, drop_cols, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Save preprocessor for later use\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(fitted_preprocessor, '../models/preprocessor.pkl')\n",
    "print(\"\\nâœ… Preprocessor saved to ../models/preprocessor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Train and compare multiple models\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ¤– MODEL TRAINING WITH MLFLOW TRACKING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set MLflow tracking\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"Attrition_Prediction_IBM\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic_Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Random_Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=3, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nðŸ“Š Training {model_name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'ROC-AUC': roc_auc\n",
    "        })\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(model.get_params())\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Log top 10 feature importances\n",
    "            importances = model.feature_importances_\n",
    "            top_indices = np.argsort(importances)[-10:]\n",
    "            for idx in top_indices:\n",
    "                if idx < len(feature_names):\n",
    "                    mlflow.log_metric(f\"imp_{feature_names[idx]}\", importances[idx])\n",
    "        \n",
    "        print(f\"   âœ… {model_name}: Acc={accuracy:.3f}, F1={f1:.3f}, AUC={roc_auc:.3f}\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“Š MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "ax1 = axes[0, 0]\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "x = np.arange(len(results_df['Model']))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = results_df[metric].values\n",
    "    ax1.bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Model Performance Comparison')\n",
    "ax1.set_xticks(x + width*2)\n",
    "ax1.set_xticklabels(results_df['Model'])\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Confusion Matrix for best model\n",
    "best_model_idx = results_df['F1 Score'].argmax()\n",
    "best_model_name = results_df.iloc[best_model_idx]['Model']\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "            xticklabels=['Stayed', 'Left'],\n",
    "            yticklabels=['Stayed', 'Left'])\n",
    "ax2.set_title(f'Confusion Matrix - {best_model_name}')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "# Plot 3: ROC Curves\n",
    "ax3 = axes[1, 0]\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    ax3.plot(fpr, tpr, label=f'{model_name} (AUC={auc:.3f})')\n",
    "\n",
    "ax3.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax3.set_xlabel('False Positive Rate')\n",
    "ax3.set_ylabel('True Positive Rate')\n",
    "ax3.set_title('ROC Curves Comparison')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Feature Importance (if best model has it)\n",
    "ax4 = axes[1, 1]\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[-10:]  # Top 10\n",
    "    \n",
    "    # Get feature names safely\n",
    "    top_features = []\n",
    "    top_importances = []\n",
    "    for idx in indices:\n",
    "        if idx < len(feature_names):\n",
    "            top_features.append(feature_names[idx])\n",
    "            top_importances.append(importances[idx])\n",
    "    \n",
    "    y_pos = np.arange(len(top_features))\n",
    "    ax4.barh(y_pos, top_importances)\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(top_features)\n",
    "    ax4.set_xlabel('Importance')\n",
    "    ax4.set_title(f'Top 10 Features - {best_model_name}')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Feature importance not available\\nfor this model type',\n",
    "             ha='center', va='center', transform=ax4.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Detailed analysis of best model\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ” DETAILED ANALYSIS - {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Predictions\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Stayed', 'Left']))\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "npv = tn / (tn + fn)  # Negative predictive value\n",
    "\n",
    "print(f\"\\nðŸ“Š Additional Metrics:\")\n",
    "print(f\"   Specificity (True Negative Rate): {specificity:.3f}\")\n",
    "print(f\"   NPV (Negative Predictive Value): {npv:.3f}\")\n",
    "print(f\"   False Positive Rate: {fp/(fp+tn):.3f}\")\n",
    "print(f\"   False Negative Rate: {fn/(fn+tp):.3f}\")\n",
    "\n",
    "# Risk stratification\n",
    "y_proba_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Probability': y_proba_best\n",
    "})\n",
    "y_proba_df['Risk_Level'] = pd.cut(y_proba_df['Probability'], \n",
    "                                 bins=[0, 0.3, 0.6, 1.0],\n",
    "                                 labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Risk Stratification:\")\n",
    "print(y_proba_df.groupby('Risk_Level')['Actual'].agg(['count', 'mean']).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c458c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Business impact simulation\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ’° BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Assumptions\n",
    "avg_salary = df['MonthlyIncome'].mean() * 12\n",
    "cost_per_hire = avg_salary * 0.5  # 50% of annual salary\n",
    "current_attrition_rate = (df['Attrition'] == 'Yes').mean()\n",
    "total_employees = len(df)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ ASSUMPTIONS:\")\n",
    "print(f\"   â€¢ Average Annual Salary: ${avg_salary:,.0f}\")\n",
    "print(f\"   â€¢ Cost per Hire (50% of salary): ${cost_per_hire:,.0f}\")\n",
    "print(f\"   â€¢ Current Attrition Rate: {current_attrition_rate*100:.1f}%\")\n",
    "print(f\"   â€¢ Total Employees: {total_employees}\")\n",
    "\n",
    "# Current cost\n",
    "current_attrition_count = (df['Attrition'] == 'Yes').sum()\n",
    "current_cost = current_attrition_count * cost_per_hire\n",
    "print(f\"\\nðŸ’¸ CURRENT ANNUAL ATTRITION COST:\")\n",
    "print(f\"   â€¢ Employees leaving: {current_attrition_count}\")\n",
    "print(f\"   â€¢ Total Cost: ${current_cost:,.0f}\")\n",
    "\n",
    "# Model impact scenarios\n",
    "print(f\"\\nðŸŽ¯ MODEL IMPACT SCENARIOS:\")\n",
    "\n",
    "# Scenario 1: 10% reduction in attrition\n",
    "reduction_10 = current_attrition_count * 0.1\n",
    "savings_10 = reduction_10 * cost_per_hire\n",
    "print(f\"\\n   ðŸ“‰ 10% Attrition Reduction:\")\n",
    "print(f\"      â€¢ Employees saved: {reduction_10:.0f}\")\n",
    "print(f\"      â€¢ Annual Savings: ${savings_10:,.0f}\")\n",
    "\n",
    "# Scenario 2: 20% reduction\n",
    "reduction_20 = current_attrition_count * 0.2\n",
    "savings_20 = reduction_20 * cost_per_hire\n",
    "print(f\"\\n   ðŸ“‰ 20% Attrition Reduction:\")\n",
    "print(f\"      â€¢ Employees saved: {reduction_20:.0f}\")\n",
    "print(f\"      â€¢ Annual Savings: ${savings_20:,.0f}\")\n",
    "\n",
    "# Scenario 3: Target high-risk only\n",
    "high_risk_count = len(y_proba_df[y_proba_df['Risk_Level'] == 'High'])\n",
    "print(f\"\\n   ðŸŽ¯ Target High-Risk Employees Only:\")\n",
    "print(f\"      â€¢ High-risk employees identified: {high_risk_count}\")\n",
    "print(f\"      â€¢ Potential annual savings: ${high_risk_count * cost_per_hire:,.0f}\")\n",
    "\n",
    "# Model performance value\n",
    "print(f\"\\nðŸ’¡ MODEL BUSINESS VALUE:\")\n",
    "print(f\"   With {best_model_name}:\")\n",
    "print(f\"   â€¢ Recall: {recall:.3f} (identifies {recall*100:.1f}% of leavers)\")\n",
    "print(f\"   â€¢ Precision: {precision:.3f} ({precision*100:.1f}% accurate predictions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Save the best model\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ’¾ SAVING BEST MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save best model\n",
    "best_model_path = '../models/best_model.pkl'\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"âœ… Model saved to: {best_model_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = '../models/feature_names.pkl'\n",
    "joblib.dump(feature_names, feature_names_path)\n",
    "print(f\"âœ… Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# Register with MLflow\n",
    "best_run = mlflow.search_runs(order_by=['metrics.f1_score DESC']).iloc[0]\n",
    "best_run_id = best_run.run_id\n",
    "print(f\"\\nðŸ“Š Best Run ID: {best_run_id}\")\n",
    "print(f\"âœ… Model registered in MLflow\")\n",
    "\n",
    "# Create model info file\n",
    "model_info = {\n",
    "    'best_model': best_model_name,\n",
    "    'accuracy': accuracy,\n",
    "    'f1_score': f1,\n",
    "    'roc_auc': roc_auc,\n",
    "    'features_count': len(feature_names),\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "model_info_path = '../models/model_info.json'\n",
    "import json\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=4)\n",
    "print(f\"âœ… Model info saved to: {model_info_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
